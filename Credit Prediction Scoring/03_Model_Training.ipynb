{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c036862e-7606-42dd-b60c-0a4701c301e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Model Training and Ensemble Creation\n",
    "\n",
    "This notebook builds upon the feature engineering from Notebook 2 to train multiple machine learning models and create an ensemble for superior credit risk prediction.\n",
    "\n",
    "### Self-Contained Implementation:\n",
    "\n",
    "Since each Databricks notebook runs independently, this notebook includes:\n",
    "- Complete data loading and feature engineering replication\n",
    "- Preprocessor pipeline setup with One-Hot Encoding\n",
    "- Training of three distinct model architectures\n",
    "- Ensemble creation combining all models\n",
    "\n",
    "### Model Architecture Strategy:\n",
    "\n",
    "We train three complementary model types:\n",
    "- **Random Forest**: Ensemble of decision trees, robust to outliers and nonlinear relationships\n",
    "- **Gradient Boosting**: Sequential tree building with error correction, high predictive accuracy  \n",
    "- **Logistic Regression**: Linear model with regularization, provides interpretable baseline\n",
    "\n",
    "### Performance Evaluation:\n",
    "- **Accuracy**: Overall correct classification rate\n",
    "- **AUC-ROC**: Area under Receiver Operating Characteristic curve, measures model's ability to distinguish between classes\n",
    "- **Ensemble Performance**: Weighted combination of all models for optimal results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56389fe2-4898-47c7-af60-9e352ddc5036",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline as SkPipeline\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "print(\"Notebook 3: Model Training and Ensemble Creation\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Recreate the feature engineering function from Notebook 2\n",
    "def create_real_time_features(df):\n",
    "    \"\"\"Create features suitable for real-time scoring\"\"\"\n",
    "    df_enhanced = df.copy()\n",
    "    \n",
    "    # Financial ratios (real-time calculable)\n",
    "    df_enhanced['Debt_to_Income_Ratio'] = df_enhanced['CCAvg'] / (df_enhanced['Income']/12 + 1e-6)\n",
    "    df_enhanced['Savings_Rate'] = (df_enhanced['Income'] - df_enhanced['CCAvg'] * 12) / df_enhanced['Income']\n",
    "    df_enhanced['Credit_Usage_Intensity'] = df_enhanced['CCAvg'] / (df_enhanced['Income']/12 + 1e-6)\n",
    "    \n",
    "    # Behavioral features\n",
    "    df_enhanced['Digital_Engagement'] = df_enhanced['Online'] + df_enhanced['CreditCard']\n",
    "    df_enhanced['Investment_Profile'] = df_enhanced['Securities Account'] + df_enhanced['CD Account']\n",
    "    \n",
    "    # Stability indicators\n",
    "    df_enhanced['Career_Stage'] = df_enhanced['Experience'] / (df_enhanced['Age'] + 1e-6)\n",
    "    df_enhanced['Family_Financial_Stress'] = df_enhanced['Family'] / (df_enhanced['Income']/1000 + 1e-6)\n",
    "    \n",
    "    # Binning for categorical encoding\n",
    "    df_enhanced['Income_Bin'] = pd.cut(df_enhanced['Income'], \n",
    "                                      bins=[0, 50, 100, 200, 500], \n",
    "                                      labels=['Low', 'Medium', 'High', 'Very High'])\n",
    "    \n",
    "    df_enhanced['Age_Group'] = pd.cut(df_enhanced['Age'],\n",
    "                                     bins=[0, 30, 45, 60, 100],\n",
    "                                     labels=['Young', 'Adult', 'Middle', 'Senior'])\n",
    "    \n",
    "    df_enhanced['CCAvg_Level'] = pd.cut(df_enhanced['CCAvg'],\n",
    "                                       bins=[0, 1, 3, 6, 10],\n",
    "                                       labels=['Low', 'Medium', 'High', 'Very High'])\n",
    "    \n",
    "    return df_enhanced\n",
    "\n",
    "# Load and process the data (same as Notebook 2)\n",
    "print(\"Loading data from Databricks...\")\n",
    "df = spark.table(\"personal_catalog.default.bank_loan_modelling\")\n",
    "pandas_df = df.toPandas()\n",
    "\n",
    "print(\"Applying feature engineering...\")\n",
    "enhanced_pandas_df = create_real_time_features(pandas_df)\n",
    "print(f\"Enhanced dataset shape: {enhanced_pandas_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d05cb4c1-7e2f-4e80-8e9b-8ac7b1697084",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define features for the model (same as Notebook 2)\n",
    "categorical_features = ['Income_Bin', 'Age_Group', 'CCAvg_Level', 'Education']\n",
    "numerical_features = [\n",
    "    'Age', 'Experience', 'Income', 'Family', 'CCAvg', 'Mortgage',\n",
    "    'Debt_to_Income_Ratio', 'Savings_Rate', 'Credit_Usage_Intensity',\n",
    "    'Digital_Engagement', 'Investment_Profile', 'Career_Stage', \n",
    "    'Family_Financial_Stress'\n",
    "]\n",
    "\n",
    "# Target variable\n",
    "target = 'Personal Loan'\n",
    "\n",
    "# Create preprocessing pipeline with One-Hot Encoding\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_features),\n",
    "        ('cat', OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore'), categorical_features)\n",
    "    ])\n",
    "\n",
    "# Create the full pipeline with multiple model options\n",
    "def create_sklearn_pipeline(model_type='random_forest'):\n",
    "    if model_type == 'random_forest':\n",
    "        model = RandomForestClassifier(\n",
    "            n_estimators=100,\n",
    "            max_depth=10,\n",
    "            random_state=42,\n",
    "            class_weight='balanced'\n",
    "        )\n",
    "    elif model_type == 'gradient_boosting':\n",
    "        model = GradientBoostingClassifier(\n",
    "            n_estimators=100,\n",
    "            max_depth=6,\n",
    "            random_state=42\n",
    "        )\n",
    "    else:  # logistic regression\n",
    "        model = LogisticRegression(\n",
    "            random_state=42,\n",
    "            class_weight='balanced',\n",
    "            max_iter=1000\n",
    "        )\n",
    "    \n",
    "    return SkPipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', model)\n",
    "    ])\n",
    "\n",
    "# Prepare data for training\n",
    "X = enhanced_pandas_df[numerical_features + categorical_features]\n",
    "y = enhanced_pandas_df[target]\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(\"Data preparation complete!\")\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "print(f\"Number of features: {X_train.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f48d415-2cb5-4a27-ae93-3a4ba6b4cb5c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Train multiple models for ensemble scoring\n",
    "models = {}\n",
    "model_performance = {}\n",
    "\n",
    "print(\"Starting model training...\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for model_name in ['random_forest', 'gradient_boosting', 'logistic_regression']:\n",
    "    print(f\"Training {model_name}...\")\n",
    "    \n",
    "    # Create and train model\n",
    "    pipeline = create_sklearn_pipeline(model_name)\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    y_pred_proba = pipeline.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Store model and performance\n",
    "    models[model_name] = pipeline\n",
    "    model_performance[model_name] = {\n",
    "        'accuracy': accuracy_score(y_test, y_pred),\n",
    "        'roc_auc': roc_auc_score(y_test, y_pred_proba)\n",
    "    }\n",
    "    \n",
    "    print(f\"{model_name} - Accuracy: {model_performance[model_name]['accuracy']:.4f}, \"\n",
    "          f\"AUC: {model_performance[model_name]['roc_auc']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74ebc45d-74b2-483a-aa3e-9ef16d8f1461",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Ensemble Model Creation\n",
    "\n",
    "This section combines the individual models into a weighted ensemble for improved performance and stability.\n",
    "\n",
    "### Ensemble Strategy:\n",
    "\n",
    "The ensemble uses weighted averaging of model predictions:\n",
    "- **Random Forest**: 40% weight (strong overall performer)\n",
    "- **Gradient Boosting**: 40% weight (high predictive power)\n",
    "- **Logistic Regression**: 20% weight (linear perspective)\n",
    "\n",
    "### Benefits of Ensemble:\n",
    "- **Reduced Variance**: Averages out individual model errors\n",
    "- **Improved Robustness**: Less sensitive to noisy data\n",
    "- **Better Generalization**: Combines different learning biases\n",
    "- **Enhanced Performance**: Typically outperforms single models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76e1004b-b0c8-4076-acad-96e26862c8ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create ensemble predictions\n",
    "def ensemble_predict_proba(X, models, weights=None):\n",
    "    if weights is None:\n",
    "        weights = [0.4, 0.4, 0.2]  # RF, GBM, LR\n",
    "    \n",
    "    predictions = []\n",
    "    for i, (name, model) in enumerate(models.items()):\n",
    "        pred_proba = model.predict_proba(X)[:, 1]\n",
    "        predictions.append(pred_proba * weights[i])\n",
    "    \n",
    "    return np.sum(predictions, axis=0)\n",
    "\n",
    "# Get ensemble predictions\n",
    "ensemble_proba = ensemble_predict_proba(X_test, models)\n",
    "ensemble_auc = roc_auc_score(y_test, ensemble_proba)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"ENSEMBLE MODEL RESULTS\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Ensemble Model AUC: {ensemble_auc:.4f}\")\n",
    "\n",
    "# Compare ensemble performance with individual models\n",
    "print(\"\\nPerformance Comparison:\")\n",
    "print(\"-\" * 30)\n",
    "for model_name in model_performance:\n",
    "    print(f\"{model_name:20} AUC: {model_performance[model_name]['roc_auc']:.4f}\")\n",
    "print(f\"{'Ensemble':20} AUC: {ensemble_auc:.4f}\")\n",
    "\n",
    "print(\"\\nModel training completed successfully!\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "03_Model_Training",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
